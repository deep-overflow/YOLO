{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93FpwWOt0wkl"
      },
      "source": [
        "# 특징\n",
        "- Classifier만 학습\n",
        " \n",
        "# TODO\n",
        "- Loss는 CPU에서 계산하는게 빠를수도 있겠다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5Lc_YFpqiT"
      },
      "source": [
        "# Library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdphfdMEI4iA",
        "outputId": "889df7b1-8517-4efe-a16e-77e9a2ecc61a"
      },
      "outputs": [],
      "source": [
        "# ! pip install wandb opencv-python-headless==4.2.0.32 albumentations==1.1.0 torch-summary timm==0.5.4 einops joblib icecream xmltodict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUOH9cLG0wkp"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BlIWINROjStc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from collections import defaultdict\n",
        "\n",
        "# XML Parsing\n",
        "import xmltodict\n",
        "import json\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as transforms\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from einops import rearrange, reduce, repeat\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "\n",
        "# Image Processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "\n",
        "# Debugging\n",
        "from icecream import ic\n",
        "\n",
        "# math, plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biNmX6CvIwZP"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ONv4iqlXIwZP"
      },
      "outputs": [],
      "source": [
        "CONFIG = dict(\n",
        "    S=7,\n",
        "    B=2,\n",
        "    C=20,\n",
        "    seed=42,\n",
        "    batch_size=4,\n",
        "    epoch = 150,\n",
        "    height=384,   # y\n",
        "    width=384,    # x\n",
        "    lambda_coord=5.,\n",
        "    lambda_noobj=0.5,\n",
        "    lr=1e-5,\n",
        "    start_epoch=1,\n",
        "    # device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    device=torch.device(\"mps\" if torch.has_mps else \"cpu\"),\n",
        ")\n",
        "# os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = \"1\"\n",
        "ic.disable()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrlBwbRu0wkr"
      },
      "source": [
        "# WANDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "B_km3BBi0wkr",
        "outputId": "20753484-8a9b-4055-ab10-b7f042331fef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "# If you don't want your script to sync to the cloud\n",
        "os.environ['WANDB_MODE'] = 'offline'\n",
        "\n",
        "run = wandb.init(project=\"YOLOv1\", entity=\"jiwon7258\", config = CONFIG, job_type='train')\n",
        "run.name = 'MPS'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTZ3bbVIwZP"
      },
      "source": [
        "# SET SEED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TSp0h58SIwZP"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "\n",
        "set_seed(CONFIG['seed'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVqje-0ptYG"
      },
      "source": [
        "# Dataset and Dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLLc7KCtIwZQ"
      },
      "source": [
        "참고 블로그\n",
        "\n",
        "-   [PASCAL VOC DATASET 설명](https://ctkim.tistory.com/190)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VGFwH6qQ0wkt"
      },
      "outputs": [],
      "source": [
        "# 픽셀값으로 되어 있는 bbox 값을 (이미지 크기에 대한) 상대값으로 변경해줍니다\n",
        "def bbox_transform(bbox: dict, img_height, img_width) -> dict:\n",
        "    bbox = bbox.copy()\n",
        "    # 기존 값을 변경\n",
        "    bbox['xmin'] = int(bbox['xmin']) / img_width        # must be float\n",
        "    bbox['xmax'] = int(bbox['xmax']) / img_width\n",
        "    bbox['ymin'] = int(bbox['ymin']) / img_height\n",
        "    bbox['ymax'] = int(bbox['ymax']) / img_height\n",
        "    bbox['x'] = (bbox['xmin'] + bbox['xmax']) / 2\n",
        "    bbox['y'] = (bbox['ymin'] + bbox['ymax']) / 2\n",
        "    bbox['w'] = bbox['xmax'] - bbox['xmin']\n",
        "    bbox['h'] = bbox['ymax'] - bbox['ymin']\n",
        "    # 새 값 추가\n",
        "    bbox['sqrt_w'] = sqrt(bbox['w'])\n",
        "    bbox['sqrt_h'] = sqrt(bbox['h'])\n",
        "\n",
        "    return bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C47Tz_Mw0wku"
      },
      "outputs": [],
      "source": [
        "def print_img(img, annotation):\n",
        "    \"\"\"  \n",
        "    {'annotation': {'filename': '000017.jpg',\n",
        "                'folder': 'VOC2007',\n",
        "                'object': [{'bndbox': {'xmax': '279',\n",
        "                                       'xmin': '185',\n",
        "                                       'ymax': '199',\n",
        "                                       'ymin': '62'},\n",
        "                            'difficult': '0',\n",
        "                            'name': 'person',\n",
        "                            'pose': 'Left',\n",
        "                            'truncated': '0'},\n",
        "                           {'bndbox': {'xmax': '403',\n",
        "                                       'xmin': '90',\n",
        "                                       'ymax': '336',\n",
        "                                       'ymin': '78'},\n",
        "                            'difficult': '0',\n",
        "                            'name': 'horse',\n",
        "                            'pose': 'Left',\n",
        "                            'truncated': '0'}],\n",
        "                'owner': {'flickrid': 'genewolf', 'name': 'whiskey kitten'},\n",
        "                'segmented': '0',\n",
        "                'size': {'depth': '3', 'height': '364', 'width': '480'},\n",
        "                'source': {'annotation': 'PASCAL VOC2007',\n",
        "                           'database': 'The VOC2007 Database',\n",
        "                           'flickrid': '228217974',\n",
        "                           'image': 'flickr'}}}\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. RGB로 바꾸어주기\n",
        "    img = img.convert('RGB')\n",
        "\n",
        "    # 2. 사각형 그리기\n",
        "    # (xmin, ymin, xmax, ymax)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    for object in annotation['annotation']['object']:\n",
        "        box = (float(object['bndbox']['xmin']), float(object['bndbox']['ymin']), float(object['bndbox']['xmax']),  float(object['bndbox']['ymax']))\n",
        "        color = tuple(np.random.randint(low= 0, high= 256, size=3))\n",
        "        width = 3\n",
        "        text_pos = ((box[0]) + width, box[1])\n",
        "\n",
        "        draw.rectangle(box, outline=color, width=width)\n",
        "        draw.text(text_pos, object['name'], color = color)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    ax.imshow(img)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lGH_GEan0wku"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(datas : list):\n",
        "    \"\"\" \n",
        "    1. 이미지들의 크기를 조정하고, 배치 단위 텐서로 변환합니다.\n",
        "    2. list of label dictionary를 반환합니다.\n",
        "    Args :\n",
        "        datas : is a list of tuple (image, dictionary)\n",
        "\n",
        "    Return :\n",
        "        batch_img (torch.Tensor) : (B, C, H, W)\n",
        "        label_list (list) : a list of dictionary\n",
        "    \"\"\"\n",
        "    img_list = list()\n",
        "    label_list = list()\n",
        "\n",
        "    for i in range(len(datas)):\n",
        "        img = np.array(datas[i][0])   # PIL Image -> numpy array, (H, W, C)\n",
        "        original_img_height = img.shape[0]\n",
        "        original_img_width = img.shape[1]\n",
        "        label = datas[i][1]     # dict\n",
        "\n",
        "        # label['original_img_height'] = original_img_height\n",
        "        # label['original_img_width'] = original_img_width\n",
        "\n",
        "        # numpy -> tensor\n",
        "        transform = A.Compose([\n",
        "            A.Resize(CONFIG['height'], CONFIG['width']),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        img_tensor = transform(image=img)['image'].to(dtype=torch.float)    # (C, H, W) Tensor\n",
        "\n",
        "        # 각 이미지에 포함되어 있는 object 정보들에 대한 list\n",
        "        original_obj_list = label['annotation']['object']\n",
        "\n",
        "        \"\"\"  \n",
        "        데이터를 가공합니다.\n",
        "        bbox 값을 (이미지 크기에 대한) 상대값으로 설정\n",
        "\n",
        "        obj_list (list) : a list of dictionary\n",
        "            e.g. obj_list[1]['name'] : 1번째 object의 name\n",
        "        \"\"\"\n",
        "        obj_list = list()\n",
        "        for obj in original_obj_list:\n",
        "            ratio_bbox = bbox_transform(\n",
        "                bbox=obj['bndbox'], img_height=original_img_height, img_width=original_img_width)\n",
        "\n",
        "            obj_info = dict()\n",
        "            obj_info['name'] = obj['name']\n",
        "            obj_info['bbox'] = ratio_bbox\n",
        "\n",
        "            obj_list.append(obj_info)\n",
        "        \n",
        "        label['annotation']['object'] = obj_list\n",
        "\n",
        "        img_list.append(img_tensor)\n",
        "        label_list.append(label)\n",
        "\n",
        "\n",
        "    batch_img = torch.stack(img_list, dim=0)\n",
        "\n",
        "    return batch_img, label_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9OTckWV10wkv"
      },
      "outputs": [],
      "source": [
        "from xml.etree.ElementTree import Element as ET_Element\n",
        "try:\n",
        "    from defusedxml.ElementTree import parse as ET_parse\n",
        "except ImportError:\n",
        "    from xml.etree.ElementTree import parse as ET_parse\n",
        "import collections\n",
        "\n",
        "\n",
        "from typing import Any, Callable, Dict, Optional, Tuple, List\n",
        "\n",
        "def parse_voc_xml(node: ET_Element) -> Dict[str, Any]:\n",
        "    # 리턴값은 일반 Dict\n",
        "    voc_dict: Dict[str, Any] = {}\n",
        "\n",
        "    # children : [자식element1, 자식element2, 자식element3, ...]\n",
        "    children = list(node)\n",
        "    if children:\n",
        "        # 중복된 태그들은 하나의 list로 관리한다\n",
        "        # e.g.) {object : [{ }, { }, { }, ... ] }\n",
        "        def_dic: Dict[str, Any] = collections.defaultdict(list)\n",
        "        for dc in map(VOCDetection.parse_voc_xml, children):\n",
        "            for ind, v in dc.items():\n",
        "                def_dic[ind].append(v)\n",
        "        if node.tag == \"annotation\":\n",
        "            def_dic[\"object\"] = [def_dic[\"object\"]]\n",
        "        voc_dict = {node.tag: {ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()}}\n",
        "    # element에 text가 있는 경우\n",
        "    if node.text:\n",
        "        # text를 얻는다\n",
        "        text = node.text.strip()\n",
        "        # 자식이 없는경우\n",
        "        if not children:\n",
        "            # {tag : text}를 voc_dict에 추가한다\n",
        "            voc_dict[node.tag] = text\n",
        "    return voc_dict\n",
        "\n",
        "class trainDataset(Dataset):\n",
        "    \"\"\"  \n",
        "    Train 커스텀 데이터셋\n",
        "    \"\"\"\n",
        "    def __init__(self, root:str ='./'):\n",
        "        self.root = root\n",
        "        self.img_list_path = Path(root) / Path('VOCdevkit/VOC2007/ImageSets/Main/trainval.txt')\n",
        "        self.img_list = list()\n",
        "        with open(self.img_list_path) as f:\n",
        "            for line in f:\n",
        "                self.img_list.append(line.strip())\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index : int):\n",
        "        \"\"\"  \n",
        "        Return :\n",
        "        - img (PIL image)\n",
        "        - annotation (dict)\n",
        "        \"\"\"\n",
        "\n",
        "        self.img_file_name = self.img_list[index]\n",
        "        self.img_path = Path(self.root) / Path('VOCdevkit/VOC2007/JPEGImages') / Path(self.img_file_name + '.jpg')\n",
        "        annotation_path = Path(self.root) / Path('VOCdevkit/VOC2007/Annotations') / Path(self.img_file_name + '.xml')\n",
        "\n",
        "        annotation_str =''\n",
        "        with open(annotation_path) as f:\n",
        "            for line in f:\n",
        "                annotation_str += line\n",
        "\n",
        "        # pprint(annotation_str)\n",
        "        \n",
        "        img = Image.open(self.img_path)\n",
        "        target = parse_voc_xml(ET_parse(annotation_path).getroot())\n",
        "\n",
        "\n",
        "        return (img, target)\n",
        "\n",
        "\n",
        "class valDataset(Dataset):\n",
        "    \"\"\"  \n",
        "    Train 커스텀 데이터셋\n",
        "    \"\"\"\n",
        "    def __init__(self, root:str ='./'):\n",
        "        self.root = root\n",
        "        self.img_list_path = Path(root) / Path('VOCdevkit/VOC2007/ImageSets/Main/val.txt')\n",
        "        self.img_list = list()\n",
        "        with open(self.img_list_path) as f:\n",
        "            for line in f:\n",
        "                self.img_list.append(line.strip())\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index : int):\n",
        "        \"\"\"  \n",
        "        Return :\n",
        "        - img (PIL image)\n",
        "        - annotation (dict)\n",
        "        \"\"\"\n",
        "\n",
        "        self.img_file_name = self.img_list[index]\n",
        "        self.img_path = Path(self.root) / Path('VOCdevkit/VOC2007/JPEGImages') / Path(self.img_file_name + '.jpg')\n",
        "        annotation_path = Path(self.root) / Path('VOCdevkit/VOC2007/Annotations') / Path(self.img_file_name + '.xml')\n",
        "\n",
        "        annotation_str =''\n",
        "        with open(annotation_path) as f:\n",
        "            for line in f:\n",
        "                annotation_str += line\n",
        "\n",
        "        # pprint(annotation_str)\n",
        "        \n",
        "        img = Image.open(self.img_path)\n",
        "        # TODO : Parse 분석하기\n",
        "        target = parse_voc_xml(ET_parse(annotation_path).getroot())\n",
        "\n",
        "        return (img, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "3463bc0c1a244f718df1539878ab64e7",
            "72e72e3257794fbd9515305b15b2c21a",
            "0be7ff29353645c39b217b30ce2a3ac0",
            "c72e79f709d8496aae8a38790fb84d02",
            "392e041667a7404f9b384e7bb416e190",
            "9feae01e3c6e4551bedc81b35cbea463",
            "696bf2b708d446109345cdee63772d72",
            "f9e39fe36cf04fa2bb1f00dacaa46b5b",
            "ccf8808af71e4c4e82e603440b59c0ae",
            "25ae4366f4d24b7faf1368d97ce6c551",
            "3299d80f126f4c938f18db3d051be54e"
          ]
        },
        "id": "7zDyMOXOkUb3",
        "outputId": "b9371228-1685-4ebe-9214-17b1e3c180c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ../datasets/VOCtrainval_06-Nov-2007.tar\n",
            "Extracting ../datasets/VOCtrainval_06-Nov-2007.tar to ../datasets\n"
          ]
        }
      ],
      "source": [
        "root_path = '../datasets'\n",
        "train_dataset_download = VOCDetection(root=root_path, year='2007',\n",
        "                       image_set='train', download=True,)\n",
        "train_dataset = trainDataset(root=root_path)\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=CONFIG['batch_size'], collate_fn=custom_collate_fn)\n",
        "val_dataset = valDataset(root=root_path)\n",
        "val_dataloader = DataLoader(dataset = val_dataset, batch_size=CONFIG['batch_size'], collate_fn=custom_collate_fn)\n",
        "\n",
        "# test_dataset_download = VOCDetection(root='./test/', year='2007', image_set='test', download=True)\n",
        "# test_dataloader = DataLoader(dataset = test_dataset_download, batch_size=CONFIG['batch_size'], collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "7jQQY0_K0wkw",
        "outputId": "99ce9806-d5b7-4e7a-e143-5b0d9d3c6147"
      },
      "outputs": [],
      "source": [
        "ic.enable()\n",
        "test_sample = trainDataset()\n",
        "img, annotation = test_sample[0]\n",
        "# print(annotation)\n",
        "print_img(img,annotation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDx7KvkmIwZR"
      },
      "source": [
        "다음 과정을 통해 라벨에 번호를 부여합니다.\n",
        "\n",
        "번호-라벨 쌍이 `labels_dict`에 저장됩니다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnm3e0bYIwZR"
      },
      "outputs": [],
      "source": [
        "labels_list = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep', 'aeroplane',\n",
        "               'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train', 'bottle',\n",
        "               'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
        "\n",
        "labels_name_dict = dict()\n",
        "name_labels_dict = dict()\n",
        "for i, label in enumerate(labels_list):\n",
        "    labels_name_dict[i] = label\n",
        "    name_labels_dict[label] = i\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1D2V90RpsD2"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_yAN4P40wkw"
      },
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "input : (3, 448, 448) = (C, H, W)\n",
        "output : S * S * (B * 5 + C)\n",
        "\"\"\"\n",
        "class Yolo(nn.Module):\n",
        "    def __init__(self, S, B, C):\n",
        "        super().__init__()\n",
        "        # 모델 상수\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        \n",
        "        # backbone\n",
        "        self.backbone = timm.create_model('swin_base_patch4_window12_384_in22k', pretrained=True)\n",
        "        self.backbone.reset_classifier(self.S * self.S * (5 * self.B + self.C))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # freeze backbone\n",
        "        freeze_exception = ['norm.weight',\n",
        "                            'norm.bias',\n",
        "                            'head.weight',\n",
        "                            'head.bias']\n",
        "\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if (name not in freeze_exception):\n",
        "                # print(name)\n",
        "                param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.backbone(x)\n",
        "        out = self.sigmoid(out).clone()\n",
        "        # out = rearrange(out, 'bs (S s X) -> bs S s X', S = self.S, s = self.S)\n",
        "        out = torch.reshape(out, (-1, self.S, self.S, 5 * self.B + self.C))\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSLYTqLiIwZS"
      },
      "outputs": [],
      "source": [
        "yolo = Yolo(S=CONFIG['S'], B=CONFIG['B'], C=CONFIG['C'])\n",
        "# pred : (S, S, (B *5 + C))\n",
        "# pred = yolo(img)\n",
        "# ic.enable()\n",
        "# yolo(torch.randn(1,3,384,384)).shape\n",
        "# summary(yolo, (3,384,384))\n",
        "yolo = yolo.to(CONFIG['device'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, param in yolo.named_parameters():\n",
        "    pprint(name + \" : \" + str(param.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x19oUQ3F7Q6X",
        "outputId": "573be5a0-5cc5-4688-b8f5-daa0d53f8dd6"
      },
      "outputs": [],
      "source": [
        "# wandb.restore('YOLOV1epoch52_Loss6.2437.bin', 'jiwon7258/YOLOv1/3kufj7nx', root='./')\n",
        "# yolo.load_state_dict(torch.load('YOLOV1epoch52_Loss6.2437.bin',\n",
        "#                       map_location=CONFIG['device']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TEl01eWIwZS"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zfDVZyHIwZS"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=yolo.parameters(), lr=CONFIG['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtHL5wW9IwZS"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GqMfIDBVIwZT",
        "outputId": "73f277cc-1f2f-40e4-ed89-35f83d1713a1"
      },
      "outputs": [],
      "source": [
        "# (x,y,w,h) tensor 둘을 받아서 IoU를 계산합니다\n",
        "def IOU(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n",
        "    \n",
        "    assert len(bbox1) == 4, 'bbox1이 이상합니다'\n",
        "    assert len(bbox2) == 4, 'bbox2가 이상합니다'\n",
        "\n",
        "    \n",
        "    max_x1 = bbox1[0] + (bbox1[2] / 2)\n",
        "    min_x1 = bbox1[0] - (bbox1[2] / 2)\n",
        "    max_y1 = bbox1[1] + (bbox1[3] / 2)\n",
        "    min_y1 = bbox1[1] - (bbox1[3] / 2)\n",
        "\n",
        "    max_x2 = bbox2[0] + (bbox2[2] / 2)\n",
        "    min_x2 = bbox2[0] - (bbox2[2] / 2)\n",
        "    max_y2 = bbox2[1] + (bbox2[3] / 2)\n",
        "    min_y2 = bbox2[1] - (bbox2[3] / 2)\n",
        "\n",
        "    # 직사각형 A, B의 넓이를 구한다.\n",
        "    # get area of rectangle A and B\n",
        "    rect1_area = (max_x1 - min_x1) * (max_y1 - min_y1)\n",
        "    rect2_area = (max_x2 - min_x2) * (max_y2 - min_y2)\n",
        "\n",
        "    # Intersection의 가로와 세로 길이를 구한다.\n",
        "    # get length and width of intersection.\n",
        "    intersection_x_length = min(max_x1, max_x2) - max(min_x1, min_x2)\n",
        "    intersection_y_length = min(max_y1, max_y2) - max(min_y1, min_y2)\n",
        "    \n",
        "    \n",
        "    # width와 length의 길이가 유효하다면 IoU를 구한다.\n",
        "    # If the width and length are valid, get IoU.\n",
        "    if (bool(intersection_x_length > 0) & bool(intersection_y_length > 0)):\n",
        "        intersection_area = intersection_x_length * intersection_y_length\n",
        "        union_area = rect1_area + rect2_area - intersection_area\n",
        "        ret = intersection_area / union_area\n",
        "    else :\n",
        "        ret = 0\n",
        "    return torch.Tensor([ret])\n",
        "\n",
        "\"\"\" \n",
        "코드 출처\n",
        "https://gaussian37.github.io/math-algorithm-iou/\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubdJ_Aq40wkx"
      },
      "outputs": [],
      "source": [
        "def sum_squared_loss(pred : torch.Tensor, target : torch.Tensor):\n",
        "    if pred.dim() == 0:\n",
        "        pred.unsqueeze_(0)\n",
        "    if target.dim() == 0:\n",
        "        target.unsqueeze_(0)\n",
        "\n",
        "    assert len(pred) == len(target), '에러 : 두 텐서의 길이가 다릅니다'\n",
        "\n",
        "    length = len(pred)\n",
        "    \n",
        "    sum = torch.Tensor([0])\n",
        "    for i in range(length):\n",
        "        sum = sum + pow((pred[i] - target[i]), 2)\n",
        "\n",
        "    return sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tti10R80wkx"
      },
      "outputs": [],
      "source": [
        "def loss_func(pred : torch.Tensor, label_list : list):\n",
        "    \"\"\"  \n",
        "    Loss를 계산합니다.\n",
        "\n",
        "    Args:\n",
        "        label_list (list): a list of dictionary\n",
        "        pred (torch.Tensor): (batch, S, S, 5*B + C) = (batch, 7, 7, 30)\n",
        "    \"\"\"\n",
        "    \n",
        "    # ============ Loss 계산 ===========\n",
        "\n",
        "    mse = torch.nn.MSELoss(reduction='sum')\n",
        "    loss_list = list()\n",
        "\n",
        "    # 각 batch 중, 하나의 데이터마다 각각 loss를 계산한다\n",
        "    for batch, label in enumerate(label_list):\n",
        "        \"\"\"  \n",
        "        Args\n",
        "            label (dict) \n",
        "                e.g. {'annotation': {'filename': '000012.jpg',\n",
        "                        'folder': 'VOC2007',\n",
        "                        'object': [{'bbox': {'h': 0.5195195195195196,\n",
        "                                            'sqrt_h': 0.7207770248277338,\n",
        "                                            'sqrt_w': 0.6244997998398398,\n",
        "                                            'w': 0.38999999999999996,\n",
        "                                            'x': 0.507,\n",
        "                                            'xmax': 0.702,\n",
        "                                            'xmin': 0.312,\n",
        "                                            'y': 0.5510510510510511,\n",
        "                                            'ymax': 0.8108108108108109,\n",
        "                                            'ymin': 0.2912912912912913},\n",
        "                                    'name': 'car'}],\n",
        "                        'owner': {'flickrid': 'KevBow', 'name': '?'},\n",
        "                        'segmented': '0',\n",
        "                        'size': {'depth': '3', 'height': '333', 'width': '500'},\n",
        "                        'source': {'annotation': 'PASCAL VOC2007',\n",
        "                                'database': 'The VOC2007 Database',\n",
        "                                'flickrid': '207539885',\n",
        "                                'image': 'flickr'}}}\n",
        "        \"\"\"\n",
        "\n",
        "        # 이미지 height, width 저장\n",
        "        img_height = CONFIG['height']\n",
        "        img_width = CONFIG['width']\n",
        "\n",
        "        # 각 이미지에 포함되어 있는 object 정보들에 대한 list\n",
        "        obj_list = label['annotation']['object']\n",
        "        num_obj = len(obj_list)\n",
        "\n",
        "        # 순회를 체크하기 위해 S X S Grid를 만든다.\n",
        "        # 기본값은 0\n",
        "        # object가 있는 cell은 1로 표시한다\n",
        "        GRID = np.zeros((CONFIG['S'], CONFIG['S']))\n",
        "\n",
        "        # ============= Object가 존재하는 Cell의 Loss를 먼저 계산한다 =================\n",
        "        for n in range(num_obj):\n",
        "\n",
        "            \"\"\"\n",
        "            Example of obj_info:\n",
        "            {'bbox': {'h': 0.93048128342246,\n",
        "                    'sqrt_h': 0.9646145776539249,\n",
        "                    'sqrt_w': 0.5709640969448079,\n",
        "                    'w': 0.326,\n",
        "                    'x': 0.20299999999999999,\n",
        "                    'xmax': 0.366,\n",
        "                    'xmin': 0.04,\n",
        "                    'y': 0.4839572192513369,\n",
        "                    'ymax': 0.9491978609625669,\n",
        "                    'ymin': 0.01871657754010695},\n",
        "            'name': 'person'}\n",
        "            \"\"\"\n",
        "            obj_info = obj_list[n]\n",
        "\n",
        "            # (i,j) cell에 떨어진다고 할 때, (i,j)를 찾는다\n",
        "            x = obj_info['bbox']['x']\n",
        "            y = obj_info['bbox']['y']\n",
        "            i = int(y * CONFIG['S'])\n",
        "            j = int(x * CONFIG['S'])\n",
        "\n",
        "            # 이미 해당 cell에 object가 있었다면 pass\n",
        "            # YOLO는 한 cell에서 단 하나의 object만을 탐지하기 때문이다\n",
        "            if (GRID[i][j] == 1):\n",
        "                continue\n",
        "            else:\n",
        "                GRID[i][j] = 1\n",
        "\n",
        "            # classification loss\n",
        "            pred_probs = pred[batch, i, j, 5*CONFIG['B']:]           # (C,) tensor\n",
        "            true_label = name_labels_dict[obj_info['name']]      # Answer Label e.g.) 4\n",
        "            label_probs = torch.zeros_like(pred_probs)\n",
        "            label_probs[true_label] = 1.0\n",
        "            # print('pred_probs : ', pred_probs)\n",
        "            # print('label_probs :', label_probs)\n",
        "            loss_list.append(mse(pred_probs, label_probs))\n",
        "\n",
        "            # print('loss_list : ', loss_list)\n",
        "\n",
        "            # Responsible한 bbox를 찾는다 : target bbox와의 IOU가 제일 큰 bbox\n",
        "            max_IOU = torch.Tensor([-1]).to('cpu')\n",
        "            reponsible_bbox_num = -1\n",
        "\n",
        "            # num_bbox 0 ~ B-1\n",
        "            for num_bbox in range(CONFIG['B']):\n",
        "                pred_coord = pred[batch, i, j, 5 * (num_bbox): 5*(num_bbox) + 4]\n",
        "                target_coord = torch.Tensor(\n",
        "                    [obj_info['bbox']['x'], obj_info['bbox']['y'], obj_info['bbox']['w'], obj_info['bbox']['h']]).to('cpu')\n",
        "                iou = IOU(target_coord, pred_coord).to('cpu')\n",
        "                if bool(iou > max_IOU):\n",
        "                    reponsible_bbox_num = num_bbox\n",
        "                    max_IOU = iou\n",
        "\n",
        "            # print('max_IOU : ', max_IOU)\n",
        "\n",
        "            for num_bbox in range(CONFIG['B']):\n",
        "                pred_coord = pred[batch, i, j, 5 * (num_bbox): 5*(num_bbox) + 4]        # (4,) tensor\n",
        "                pred_confidence = pred[batch, i, j, 5*(num_bbox) + 4].unsqueeze(0)                   # (1,) tensor\n",
        "                # print('pred_confidence : ', pred_confidence.item())\n",
        "\n",
        "                # responsible한 bbox의 경우\n",
        "                # TODO inplace 연산 없이 sqrt로 바꾸기\n",
        "                if (num_bbox == reponsible_bbox_num):\n",
        "                    sqrt_pred_coord = torch.zeros_like(pred_coord)\n",
        "                    sqrt_pred_coord[0] = pred_coord[0]\n",
        "                    sqrt_pred_coord[1] = pred_coord[1]\n",
        "                    sqrt_pred_coord[2] = sqrt(pred_coord[2])     # w -> sqrt(w)\n",
        "                    sqrt_pred_coord[3] = sqrt(pred_coord[3])     # h -> sqrt(h)\n",
        "\n",
        "                    target_coord = torch.Tensor(\n",
        "                        [obj_info['bbox']['x'], obj_info['bbox']['y'], obj_info['bbox']['sqrt_w'], obj_info['bbox']['sqrt_h']])\n",
        "                    target_coord = target_coord.to('cpu')\n",
        "\n",
        "                    # localization(coordinate) loss\n",
        "                    assert pred_coord.size() == target_coord.size()\n",
        "                    loss_list.append(CONFIG['lambda_coord'] * \\\n",
        "                        mse(sqrt_pred_coord, target_coord))\n",
        "                    # confidence error loss\n",
        "                    assert pred_confidence.size() == max_IOU.size()\n",
        "                    loss_list.append(mse(pred_confidence, max_IOU))\n",
        "\n",
        "                # reponsible하지 않은 bbox의 경우\n",
        "                else:\n",
        "                    # If no object exists in cell, the  confidence scores should be zero\n",
        "                    loss_list.append(CONFIG['lambda_noobj'] * mse(pred_confidence, torch.Tensor([0]).to('cpu')))\n",
        "\n",
        "        # =========== Object가 존재하지 않는 Cell의 Loss ===========\n",
        "\n",
        "        for i in range(CONFIG['S']):\n",
        "            for j in range(CONFIG['S']):\n",
        "                # 이미 loss를 계산했던 Cell인지 확인\n",
        "                if (GRID[i][j] == 1):\n",
        "                    continue\n",
        "                else:\n",
        "                    GRID[i][j] = 1\n",
        "\n",
        "                for num_bbox in range(CONFIG['B']):\n",
        "                    pred_confidence = pred[batch, i, j, 5*(num_bbox) + 4].unsqueeze(0)\n",
        "                    loss_list.append(CONFIG['lambda_noobj'] * \\\n",
        "                        mse(pred_confidence, torch.Tensor([0]).to('cpu')))\n",
        "\n",
        "    # print('loss_list : ', loss_list)\n",
        "    loss = torch.sum(torch.stack(loss_list))  / len(label_list)\n",
        "    \n",
        "    return loss\n",
        "                    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorIoT3bIwZT"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAj4r-2n0wkx"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train_one_epoch(epoch, dataloader, scheduler, device = 'cpu', batch_size = 1) :\n",
        "    yolo.train()\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "    loss_sum = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step, (img, label) in bar:\n",
        "        \"\"\"  \n",
        "        Args :\n",
        "            img (torch.Tensor): (B, C, H, W)\n",
        "            label (list) : list of dictionary\n",
        "        \"\"\"\n",
        "        \n",
        "        img = img.to(CONFIG['device'])\n",
        "        pred = yolo(img)        # (batch, 7, 7, 30)\n",
        "        # pred = rearrange(pred, '1 a b c -> a b c')\n",
        "\n",
        "        # print(pred.requires_grad)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = pred.to('cpu')\n",
        "        loss = loss_func(pred=pred, label_list=label)\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        epoch_loss = loss_sum / (step + 1)\n",
        "            \n",
        "        bar.set_postfix(Epoch = epoch, Train_Loss = epoch_loss, LR=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return epoch_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDNainAL0wky"
      },
      "outputs": [],
      "source": [
        "def val_one_epoch(epoch, dataloader, device, batch_size):\n",
        "    yolo.eval()\n",
        "\n",
        "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
        "\n",
        "    loss_sum : int = 0\n",
        "    epoch_loss : int = 0\n",
        "\n",
        "    for step, (img, label) in bar:\n",
        "        img = img.to(device)\n",
        "        \n",
        "        pred = yolo(img)\n",
        "\n",
        "        pred = pred.to('cpu')\n",
        "\n",
        "        loss = loss_func(pred = pred, label_list = label)\n",
        "\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        epoch_loss = loss_sum / (step + 1)\n",
        "\n",
        "        bar.set_postfix(Epoch = epoch, Valid_Loss = epoch_loss)\n",
        "    \n",
        "    return epoch_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tAvZKw6IwZT",
        "outputId": "c6d970f4-95b0-451f-b9cc-159aa812209c"
      },
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(False)\n",
        "\n",
        "\n",
        "def run_training():\n",
        "    Epoch = CONFIG['epoch']\n",
        "\n",
        "    # To log gradient\n",
        "    wandb.watch(yolo)\n",
        "\n",
        "    best_loss = np.inf\n",
        "    history = defaultdict(list)\n",
        "    START_EPOCH = CONFIG['start_epoch']\n",
        "    FINETUNE_EPOCH = 5\n",
        "\n",
        "    # Fine Tuning for 3 epoch\n",
        "    for epoch in range(START_EPOCH, FINETUNE_EPOCH):\n",
        "        train_loss = train_one_epoch(epoch=epoch, dataloader = train_dataloader, scheduler = None, device=CONFIG['device'], batch_size=CONFIG['batch_size'])\n",
        "\n",
        "        val_loss = val_one_epoch(epoch = epoch, dataloader = val_dataloader, device = CONFIG['device'], batch_size= CONFIG['batch_size'])\n",
        "\n",
        "        history['Train Loss'].append(train_loss)\n",
        "        history['Valid Loss'].append(val_loss)\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "            'Train Loss' : train_loss,\n",
        "            'Valid Loss' : val_loss,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if val_loss < best_loss :\n",
        "            best_loss = val_loss\n",
        "            file_prefix = 'YOLOV1'\n",
        "            save_path = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(\n",
        "                file_prefix, epoch, best_loss)\n",
        "            torch.save(yolo.state_dict(), save_path)\n",
        "            wandb.save(save_path)\n",
        "\n",
        "    for name, param in yolo.named_parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    for epoch in range(FINETUNE_EPOCH, Epoch):\n",
        "        train_loss = train_one_epoch(epoch=epoch, dataloader = train_dataloader, scheduler = None, device=CONFIG['device'], batch_size=CONFIG['batch_size'])\n",
        "\n",
        "        val_loss = val_one_epoch(epoch = epoch, dataloader = val_dataloader, device = CONFIG['device'], batch_size= CONFIG['batch_size'])\n",
        "\n",
        "        history['Train Loss'].append(train_loss)\n",
        "        history['Valid Loss'].append(val_loss)\n",
        "\n",
        "        wandb.log(\n",
        "            {\n",
        "            'Train Loss' : train_loss,\n",
        "            'Valid Loss' : val_loss,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        if val_loss < best_loss :\n",
        "            best_loss = val_loss\n",
        "            file_prefix = 'YOLOV1'\n",
        "            save_path = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(\n",
        "                file_prefix, epoch, best_loss)\n",
        "            torch.save(yolo.state_dict(), save_path)\n",
        "            wandb.save(save_path)\n",
        "\n",
        "    \n",
        "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
        "\n",
        "\n",
        "ic.disable()\n",
        "print('### Using Device {0} ###'.format(str(CONFIG['device'])))\n",
        "run_training()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "YOLO_batchLoss.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0be7ff29353645c39b217b30ce2a3ac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9e39fe36cf04fa2bb1f00dacaa46b5b",
            "max": 460032000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccf8808af71e4c4e82e603440b59c0ae",
            "value": 460032000
          }
        },
        "25ae4366f4d24b7faf1368d97ce6c551": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3299d80f126f4c938f18db3d051be54e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3463bc0c1a244f718df1539878ab64e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72e72e3257794fbd9515305b15b2c21a",
              "IPY_MODEL_0be7ff29353645c39b217b30ce2a3ac0",
              "IPY_MODEL_c72e79f709d8496aae8a38790fb84d02"
            ],
            "layout": "IPY_MODEL_392e041667a7404f9b384e7bb416e190"
          }
        },
        "392e041667a7404f9b384e7bb416e190": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696bf2b708d446109345cdee63772d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72e72e3257794fbd9515305b15b2c21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9feae01e3c6e4551bedc81b35cbea463",
            "placeholder": "​",
            "style": "IPY_MODEL_696bf2b708d446109345cdee63772d72",
            "value": "100%"
          }
        },
        "9feae01e3c6e4551bedc81b35cbea463": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c72e79f709d8496aae8a38790fb84d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25ae4366f4d24b7faf1368d97ce6c551",
            "placeholder": "​",
            "style": "IPY_MODEL_3299d80f126f4c938f18db3d051be54e",
            "value": " 460032000/460032000 [00:16&lt;00:00, 29264018.17it/s]"
          }
        },
        "ccf8808af71e4c4e82e603440b59c0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9e39fe36cf04fa2bb1f00dacaa46b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
