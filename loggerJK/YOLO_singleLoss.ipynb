{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TODO\n",
        "- Support Mini Batch Training\n",
        "- Use Custom dataset\n",
        "- NMS (Non Maximal Suppression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5Lc_YFpqiT"
      },
      "source": [
        "# Library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EdphfdMEI4iA"
      },
      "outputs": [],
      "source": [
        "# ! pip install wandb opencv-python-headless==4.2.0.32 albumentations==1.1.0 torch-summary timm==0.5.4 einops joblib icecream\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BlIWINROjStc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as transforms\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from einops import rearrange, reduce, repeat\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "\n",
        "# Image Processing\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "\n",
        "# Debugging\n",
        "from icecream import ic\n",
        "\n",
        "# math, plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "import random\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biNmX6CvIwZP"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ONv4iqlXIwZP"
      },
      "outputs": [],
      "source": [
        "CONFIG = dict(\n",
        "    S=7,\n",
        "    B=2,\n",
        "    C=20,\n",
        "    seed = 42,\n",
        "    batch_size=1,\n",
        "    height=384,   # y\n",
        "    width=384,    # x\n",
        "    lambda_coord=5,\n",
        "    lambda_noobj=0.5,\n",
        "    lr=1e-5,\n",
        "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        ")\n",
        "ic.disable()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpTZ3bbVIwZP"
      },
      "source": [
        "# SET SEED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TSp0h58SIwZP"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=42):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "\n",
        "set_seed(CONFIG['seed'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVqje-0ptYG"
      },
      "source": [
        "# Dataset and Dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLLc7KCtIwZQ"
      },
      "source": [
        "참고할 블로그\n",
        "\n",
        "-   [PASCAL VOC DATASET 설명](https://ctkim.tistory.com/190)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch):\n",
        "    print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zDyMOXOkUb3",
        "outputId": "e56229bd-f9e0-49ee-8ce9-92463c82a8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./VOCtrainval_06-Nov-2007.tar\n",
            "Extracting ./VOCtrainval_06-Nov-2007.tar to ./\n"
          ]
        }
      ],
      "source": [
        "# class myVOC(VOCDetection):\n",
        "dataset = VOCDetection(root='./', year='2007',\n",
        "                       image_set='train', download=True,)\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=2, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(<PIL.Image.Image image mode=RGB size=500x333 at 0x17CEE7C70>, {'annotation': {'folder': 'VOC2007', 'filename': '000012.jpg', 'source': {'database': 'The VOC2007 Database', 'annotation': 'PASCAL VOC2007', 'image': 'flickr', 'flickrid': '207539885'}, 'owner': {'flickrid': 'KevBow', 'name': '?'}, 'size': {'width': '500', 'height': '333', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'car', 'pose': 'Rear', 'truncated': '0', 'difficult': '0', 'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}}), (<PIL.Image.Image image mode=RGB size=480x364 at 0x110410760>, {'annotation': {'folder': 'VOC2007', 'filename': '000017.jpg', 'source': {'database': 'The VOC2007 Database', 'annotation': 'PASCAL VOC2007', 'image': 'flickr', 'flickrid': '228217974'}, 'owner': {'flickrid': 'genewolf', 'name': 'whiskey kitten'}, 'size': {'width': '480', 'height': '364', 'depth': '3'}, 'segmented': '0', 'object': [{'name': 'person', 'pose': 'Left', 'truncated': '0', 'difficult': '0', 'bndbox': {'xmin': '185', 'ymin': '62', 'xmax': '279', 'ymax': '199'}}, {'name': 'horse', 'pose': 'Left', 'truncated': '0', 'difficult': '0', 'bndbox': {'xmin': '90', 'ymin': '78', 'xmax': '403', 'ymax': '336'}}]}})]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/jiwonkang/github/YOLO/loggerJK/YOLO custom.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jiwonkang/github/YOLO/loggerJK/YOLO%20custom.ipynb#ch0000029?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m dataloader :\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiwonkang/github/YOLO/loggerJK/YOLO%20custom.ipynb#ch0000029?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jiwonkang/github/YOLO/loggerJK/YOLO%20custom.ipynb#ch0000029?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(y)\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ],
      "source": [
        "for x,y in dataloader :\n",
        "    print(x)\n",
        "    print(y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67UC9stXnZ0g",
        "outputId": "a64583da-d27e-4a4f-ade8-acb1a980e955"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=334x500 at 0x40403EA90>,\n",
              " {'annotation': {'folder': 'VOC2007',\n",
              "   'filename': '000023.jpg',\n",
              "   'source': {'database': 'The VOC2007 Database',\n",
              "    'annotation': 'PASCAL VOC2007',\n",
              "    'image': 'flickr',\n",
              "    'flickrid': '220208496'},\n",
              "   'owner': {'flickrid': 'thevelodrome.com', 'name': '?'},\n",
              "   'size': {'width': '334', 'height': '500', 'depth': '3'},\n",
              "   'segmented': '0',\n",
              "   'object': [{'name': 'bicycle',\n",
              "     'pose': 'Unspecified',\n",
              "     'truncated': '1',\n",
              "     'difficult': '0',\n",
              "     'bndbox': {'xmin': '9', 'ymin': '230', 'xmax': '245', 'ymax': '500'}},\n",
              "    {'name': 'bicycle',\n",
              "     'pose': 'Frontal',\n",
              "     'truncated': '1',\n",
              "     'difficult': '0',\n",
              "     'bndbox': {'xmin': '230', 'ymin': '220', 'xmax': '334', 'ymax': '500'}},\n",
              "    {'name': 'bicycle',\n",
              "     'pose': 'Unspecified',\n",
              "     'truncated': '1',\n",
              "     'difficult': '1',\n",
              "     'bndbox': {'xmin': '2', 'ymin': '178', 'xmax': '90', 'ymax': '500'}},\n",
              "    {'name': 'person',\n",
              "     'pose': 'Unspecified',\n",
              "     'truncated': '1',\n",
              "     'difficult': '0',\n",
              "     'bndbox': {'xmin': '2', 'ymin': '1', 'xmax': '117', 'ymax': '369'}},\n",
              "    {'name': 'person',\n",
              "     'pose': 'Unspecified',\n",
              "     'truncated': '1',\n",
              "     'difficult': '0',\n",
              "     'bndbox': {'xmin': '3', 'ymin': '2', 'xmax': '243', 'ymax': '462'}},\n",
              "    {'name': 'person',\n",
              "     'pose': 'Unspecified',\n",
              "     'truncated': '1',\n",
              "     'difficult': '0',\n",
              "     'bndbox': {'xmin': '225', 'ymin': '1', 'xmax': '334', 'ymax': '486'}}]}})"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.__getitem__(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDx7KvkmIwZR"
      },
      "source": [
        "다음 과정을 통해 라벨에 번호를 부여합니다.\n",
        "\n",
        "번호-라벨 쌍이 `labels_dict`에 저장됩니다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "pnm3e0bYIwZR"
      },
      "outputs": [],
      "source": [
        "labels_list = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep', 'aeroplane',\n",
        "               'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train', 'bottle',\n",
        "               'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
        "\n",
        "labels_name_dict = dict()\n",
        "name_labels_dict = dict()\n",
        "for i, label in enumerate(labels_list):\n",
        "    labels_name_dict[i] = label\n",
        "    name_labels_dict[label] = i\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1D2V90RpsD2"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "input : (3, 448, 448) - (C, H, W)\n",
        "output : S * S * (B * 5 + C)\n",
        "\"\"\"\n",
        "\n",
        "# TODO : 적당한 backbone 모델로 변경하기\n",
        "class Yolo(nn.Module):\n",
        "    def __init__(self, S, B, C):\n",
        "        super().__init__()\n",
        "        # 모델 상수\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        \n",
        "        # backbone\n",
        "        self.backbone = timm.create_model('tf_efficientnetv2_s_in21ft1k', pretrained=True)\n",
        "        # freeze backbone\n",
        "        # for param in self.backbone.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        self.backbone.reset_classifier(self.S * self.S * (5 * self.B + self.C))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.backbone(x)\n",
        "        out = self.sigmoid(out).clone()\n",
        "        out = rearrange(out, 'bs (S s X) -> bs S s X', S = self.S, s = self.S)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "wSLYTqLiIwZS"
      },
      "outputs": [],
      "source": [
        "yolo = Yolo(S=CONFIG['S'], B=CONFIG['B'], C=CONFIG['C'])\n",
        "# pred : (S, S, (B *5 + C))\n",
        "# pred = yolo(img)\n",
        "# ic.enable()\n",
        "# yolo(torch.randn(1,3,384,384)).shape\n",
        "# summary(yolo, (3,384,384))\n",
        "yolo = yolo.to(CONFIG['device'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TEl01eWIwZS"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "_zfDVZyHIwZS"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=yolo.parameters(), lr=CONFIG['lr'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtHL5wW9IwZS"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "NZ0xvD17IwZS"
      },
      "outputs": [],
      "source": [
        "# 픽셀값으로 되어 있는 bbox 값을 (이미지 크기에 대한) 상대값으로 변경해줍니다\n",
        "def bbox_transform(bbox: dict, img_height, img_width) -> dict:\n",
        "    # ic(bbox['xmin'])\n",
        "    bbox['xmin'] = int(bbox['xmin']) / img_width        # must be float\n",
        "    bbox['xmax'] = int(bbox['xmax']) / img_width\n",
        "    bbox['ymin'] = int(bbox['ymin']) / img_height\n",
        "    bbox['ymax'] = int(bbox['ymax']) / img_height\n",
        "    bbox['x'] = (bbox['xmin'] + bbox['xmax']) / 2\n",
        "    bbox['y'] = (bbox['ymin'] + bbox['ymax']) / 2\n",
        "    bbox['w'] = bbox['xmax'] - bbox['xmin']\n",
        "    bbox['h'] = bbox['ymax'] - bbox['ymin']\n",
        "    bbox['sqrt_w'] = sqrt(bbox['w'])\n",
        "    bbox['sqrt_h'] = sqrt(bbox['h'])\n",
        "\n",
        "    return bbox\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GqMfIDBVIwZT",
        "outputId": "da326845-bd39-4ae1-c0c5-d5240c50c1b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' \\n코드 출처\\nhttps://gaussian37.github.io/math-algorithm-iou/\\n'"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# (x,y,w,h) tensor 둘을 받아서 IoU를 계산합니다\n",
        "def IOU(bbox1: torch.Tensor, bbox2: torch.Tensor) -> torch.Tensor:\n",
        "    assert len(bbox1) == 4, 'bbox1이 이상합니다'\n",
        "    assert len(bbox2) == 4, 'bbox2가 이상합니다'\n",
        "\n",
        "    max_x1 = bbox1[0] + (bbox1[2] / 2)\n",
        "    min_x1 = bbox1[0] - (bbox1[2] / 2)\n",
        "    max_y1 = bbox1[1] + (bbox1[3] / 2)\n",
        "    min_y1 = bbox1[1] - (bbox1[3] / 2)\n",
        "\n",
        "    max_x2 = bbox2[0] + (bbox2[2] / 2)\n",
        "    min_x2 = bbox2[0] - (bbox2[2] / 2)\n",
        "    max_y2 = bbox2[1] + (bbox2[3] / 2)\n",
        "    min_y2 = bbox2[1] - (bbox2[3] / 2)\n",
        "\n",
        "    # 직사각형 A, B의 넓이를 구한다.\n",
        "    # get area of rectangle A and B\n",
        "    rect1_area = (max_x1 - min_x1) * (max_y1 - min_y1)\n",
        "    rect2_area = (max_x2 - min_x2) * (max_y2 - min_y2)\n",
        "\n",
        "    # Intersection의 가로와 세로 길이를 구한다.\n",
        "    # get length and width of intersection.\n",
        "    intersection_x_length = min(max_x1, max_x2) - max(min_x1, min_x2)\n",
        "    intersection_y_length = min(max_y1, max_y2) - max(min_y1, min_y2)\n",
        "    \n",
        "    \n",
        "    # width와 length의 길이가 유효하다면 IoU를 구한다.\n",
        "    # If the width and length are valid, get IoU.\n",
        "    if (bool(intersection_x_length > 0) & bool(intersection_y_length > 0)):\n",
        "        intersection_area = intersection_x_length * intersection_y_length\n",
        "        union_area = rect1_area + rect2_area - intersection_area\n",
        "        ret = intersection_area / union_area\n",
        "    else :\n",
        "        ret = 0\n",
        "    return torch.Tensor([ret]).to(CONFIG['device'])\n",
        "\n",
        "\"\"\" \n",
        "코드 출처\n",
        "https://gaussian37.github.io/math-algorithm-iou/\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_squared_loss(pred : torch.Tensor, target : torch.Tensor):\n",
        "    if pred.dim() == 0:\n",
        "        pred.unsqueeze_(0)\n",
        "    if target.dim() == 0:\n",
        "        target.unsqueeze_(0)\n",
        "    assert len(pred) == len(target), '에러 : 두 텐서의 길이가 다릅니다'\n",
        "    length = len(pred)\n",
        "    sum = 0\n",
        "    for i in range(length):\n",
        "        sum += pow((pred[i] - target[i]), 2)\n",
        "    return sum.unsqueeze(0).to(CONFIG['device'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_func(data, pred : torch.Tensor):\n",
        "\n",
        "    pil_image = data[0]     # PIL 이미지\n",
        "    numpy_image = np.array(pil_image)   # (H, W, C)\n",
        "\n",
        "    # 이미지 height, width 저장\n",
        "    img_height = numpy_image.shape[0]\n",
        "    img_width = numpy_image.shape[1]\n",
        "\n",
        "    # 각 이미지에 포함되어 있는 object 정보들에 대한 리스트\n",
        "    original_obj_list = data[1]['annotation']['object']\n",
        "\n",
        "    # 데이터 가공\n",
        "    # bbox 값을 (이미지 크기에 대한) 상대값으로 설정\n",
        "    # 딕셔너리의 리스트\n",
        "    obj_list = list()\n",
        "    for obj in original_obj_list:\n",
        "        ratio_bbox = bbox_transform(\n",
        "            bbox=obj['bndbox'], img_height=img_height, img_width=img_width)\n",
        "\n",
        "        obj_info = dict()\n",
        "        obj_info['name'] = obj['name']\n",
        "        obj_info['bbox'] = ratio_bbox\n",
        "\n",
        "        obj_list.append(obj_info)\n",
        "\n",
        "    num_obj = len(obj_list)\n",
        "    ic(num_obj)\n",
        "\n",
        "    # ============ Loss 계산 ===========\n",
        "    loss_list = list()\n",
        "\n",
        "    # ============= Object가 존재하는 Cell의 Loss를 먼저 계산한다 =================\n",
        "    for n in range(num_obj):\n",
        "\n",
        "        \"\"\"\n",
        "        Example of obj_info:\n",
        "        {'bbox': {'h': 0.56,\n",
        "                'w': 0.31137724550898205,\n",
        "                'x': 0.844311377245509,\n",
        "                'xmax': 1.0,\n",
        "                'xmin': 0.688622754491018,\n",
        "                'y': 0.72,\n",
        "                'ymax': 1.0,\n",
        "                'ymin': 0.44},\n",
        "        'name': 'bicycle'},\n",
        "        \"\"\"\n",
        "        obj_info = obj_list[n]\n",
        "\n",
        "        # (i,j) cell에 떨어진다고 할 때, (i,j)를 찾는다\n",
        "        x = obj_info['bbox']['x']\n",
        "        y = obj_info['bbox']['y']\n",
        "        i = int(y * CONFIG['S'])\n",
        "        j = int(x * CONFIG['S'])\n",
        "\n",
        "        # 순회를 체크하기 위해 S X S Grid를 만든다.\n",
        "        # 기본값은 0\n",
        "        # object가 있는 cell은 1로 표시한다\n",
        "        GRID = np.zeros((CONFIG['S'], CONFIG['S']))\n",
        "\n",
        "        # 이미 해당 cell에 object가 있었다면 pass\n",
        "        # YOLO는 한 cell에서 단 하나의 object만을 탐지하기 때문이다\n",
        "        if (GRID[i][j] == 1):\n",
        "            continue\n",
        "        else:\n",
        "            GRID[i][j] = 1\n",
        "\n",
        "        # classification loss\n",
        "        pred_probs = pred[i, j, 5*CONFIG['B']:]           # (C,) tensor\n",
        "        label = name_labels_dict[obj_info['name']]      # Answer Label e.g.) 4\n",
        "        label_probs = torch.zeros_like(pred_probs)\n",
        "        label_probs[label] = 1.0\n",
        "        \n",
        "        loss_list.append(sum_squared_loss(pred_probs, label_probs))\n",
        "\n",
        "        \n",
        "        # Responsible한 bbox를 찾는다 : target bbox와의 IOU가 제일 큰 bbox\n",
        "        max_IOU = torch.Tensor([-1]).to(CONFIG['device'])\n",
        "        reponsible_bbox_num = -1\n",
        "        # num_bbox 0 ~ B-1\n",
        "        for num_bbox in range(CONFIG['B']):\n",
        "            pred_coord = pred[i, j, 5 * (num_bbox): 5*(num_bbox) + 4]\n",
        "            target_coord = torch.Tensor(\n",
        "                [obj_info['bbox']['x'], obj_info['bbox']['y'], obj_info['bbox']['w'], obj_info['bbox']['h']]).to(CONFIG['device'])\n",
        "            iou = IOU(target_coord, pred_coord).to(CONFIG['device'])\n",
        "            if bool(iou > max_IOU):\n",
        "                reponsible_bbox_num = num_bbox\n",
        "                max_IOU = iou\n",
        "\n",
        "        for num_bbox in range(CONFIG['B']):\n",
        "            pred_coord = pred[i, j, 5 * (num_bbox): 5*(num_bbox) + 4]\n",
        "            pred_confidence = pred[i, j, 5*(num_bbox) + 4]\n",
        "\n",
        "            # responsible한 bbox의 경우\n",
        "            if num_bbox == reponsible_bbox_num:\n",
        "                pred_coord[2] = sqrt(pred_coord[2])     # w -> sqrt(w)\n",
        "                pred_coord[3] = sqrt(pred_coord[3])     # h -> sqrt(h)\n",
        "\n",
        "                target_coord = torch.Tensor(\n",
        "                    [obj_info['bbox']['x'], obj_info['bbox']['y'], obj_info['bbox']['sqrt_w'], obj_info['bbox']['sqrt_h']])\n",
        "                target_coord = target_coord.to(CONFIG['device'])\n",
        "\n",
        "                # localization(coordinate) loss\n",
        "                loss_list.append( CONFIG['lambda_coord'] * \\\n",
        "                    sum_squared_loss(pred_coord, target_coord))\n",
        "                # confidence error loss\n",
        "                loss_list.append(sum_squared_loss(pred_confidence, max_IOU))\n",
        "\n",
        "            # reponsible하지 않은 bbox의 경우\n",
        "            else:\n",
        "                # If no object exists in cell, the  confidence scores should be zero\n",
        "                loss_list.append(CONFIG['lambda_noobj'] * sum_squared_loss(pred_confidence, torch.Tensor([0]).to(CONFIG['device'])))\n",
        "\n",
        "    # =========== Object가 존재하지 않는 Cell의 Loss ===========\n",
        "\n",
        "    for i in range(CONFIG['S']):\n",
        "        for j in range(CONFIG['S']):\n",
        "            # 이미 loss를 계산했던 Cell인지 확인\n",
        "            if (GRID[i][j] == 1):\n",
        "                continue\n",
        "            else:\n",
        "                GRID[i][j] = 1\n",
        "\n",
        "            for num_bbox in range(CONFIG['B']):\n",
        "                pred_confidence = pred[i, j, 5*(num_bbox) + 4]\n",
        "                loss_list.append(CONFIG['lambda_noobj'] * \\\n",
        "                    sum_squared_loss(pred_confidence, torch.Tensor([0]).to(CONFIG['device'])))\n",
        "\n",
        "    loss = torch.sum(torch.cat(loss_list))\n",
        "    return loss\n",
        "                    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aorIoT3bIwZT"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "def train_one_epoch(epoch, device = 'cpu', batch_size = 1) :\n",
        "    yolo.train()\n",
        "\n",
        "    bar = tqdm(enumerate(dataset), total=len(dataset))\n",
        "\n",
        "    loss_sum = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for step, data in bar:\n",
        "\n",
        "        # 이미지 불러오기\n",
        "        pil_image = data[0]     # PIL 이미지\n",
        "        numpy_image = np.array(pil_image)   # (H, W, C)\n",
        "        opencv_image = cv2.cvtColor(numpy_image, cv2.IMREAD_COLOR)  # (H, W, C)\n",
        "\n",
        "        # 이미지 확인\n",
        "        # TODO: 바운딩 박스까지 표시하는 이미지로 바꾸기\n",
        "        plt.imshow(numpy_image)\n",
        "\n",
        "        # numpy -> tensor\n",
        "        transform = A.Compose([\n",
        "            A.Resize(CONFIG['height'], CONFIG['width']),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        transformed = transform(image=opencv_image)[\n",
        "            'image']      # (C, H, W) Tensor\n",
        "        transformed = rearrange(transformed, 'c h w -> 1 c h w').float()\n",
        "        \n",
        "        ic(transformed.shape)\n",
        "        \n",
        "        transformed = transformed.to(CONFIG['device'])\n",
        "        pred = yolo(transformed)        # (1, 7, 7, 30)\n",
        "        pred = rearrange(pred, '1 a b c -> a b c')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss_func(data, pred)\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        # for name, param in yolo.named_parameters():\n",
        "        #     ic(name)\n",
        "        #     ic(torch.sum(param.grad))\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        epoch_loss = loss_sum / (step + 1)\n",
        "            \n",
        "        bar.set_postfix(Epoch = epoch, Loss = epoch_loss)\n",
        "        # break\n",
        "\n",
        "    return epoch_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1tAvZKw6IwZT",
        "outputId": "28831e6f-893c-42bd-ac51-e1778c46bdb5"
      },
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(False)\n",
        "def run_training():\n",
        "    Epoch = 100\n",
        "\n",
        "    best_loss = np.inf\n",
        "    for epoch in range(Epoch):\n",
        "        epoch_loss = train_one_epoch(epoch=epoch, device=CONFIG['device'], batch_size=CONFIG['batch_size']])\n",
        "\n",
        "        if epoch_loss < best_loss :\n",
        "            best_loss = epoch_loss\n",
        "            file_prefix = 'YOLOV1'\n",
        "            save_path = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(\n",
        "                file_prefix, epoch, best_loss)\n",
        "            torch.save(yolo.state_dict(), save_path)\n",
        "\n",
        "\n",
        "ic.disable()\n",
        "print('### Using Device {0} ###'.format(str(CONFIG['device'])))\n",
        "run_training()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "YOLO.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
